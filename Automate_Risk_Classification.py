# -*- coding: utf-8 -*-
"""Automate_Risk_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lfm9nWRJp-E7J4lR0iOSddJKIeRyC1tN

# pip installs
"""

!pip install transformers datasets wandb evaluate lime gradio

"""# Setup"""

import wandb
wandb.login()
# You can find your API key in your browser here: https://wandb.ai/authorize

from google.colab import drive
drive.mount('/content/drive')

"""# Datasets

## EU AI Act Dataset
"""

import pandas as pd
import ast

# Load the EU AI Act CSV
df_eu = pd.read_csv('/content/drive/MyDrive/Automating-Risk-Evaluation/EU_AI_Act_Dataset/eu_ai_act_2024_pdf_with_payload_v2.csv')

df_eu["meta_data"] = df_eu["meta_data"].apply(ast.literal_eval)

# Extract the most relevant legal identifier from the metadata
def extract_label(meta):
    for item in meta[::-1]: 
        if item.get("article"):
            return f'{item["article"]}: {item.get("article_title", "")}'.strip()
        elif item.get("chapter"):
            return f'{item["chapter"]}: {item.get("chapter_title", "")}'.strip()
    return "Unknown"

df_eu["labels"] = df_eu["meta_data"].apply(extract_label)
df_eu_final = df_eu[["text", "labels"]].dropna().reset_index(drop=True)
df_eu_final

df_filtered = pd.DataFrame()
for index, row in df_eu_final.iterrows():
    if row['labels'] != 'Unknown':
        df_filtered = pd.concat([df_filtered, pd.DataFrame(row).T], ignore_index=True)

df_eu_final = df_filtered
df_eu_final

unique_labels = sorted(df_eu_final["labels"].unique())

label2id = {label: idx for idx, label in enumerate(unique_labels)}
id2label = {idx: label for label, idx in label2id.items()}

df_eu_final["labels"] = df_eu_final["labels"].map(label2id)

print("Label to ID mapping:")
print(label2id)
print("\nID to Label mapping:")
print(id2label)

df_eu_final

"""## Mini Dataset Creation"""

import pandas as pd
import random

# Example templates per category
templates = {
    "unacceptable": [
        "An AI system that assigns a social score based on {data_source}.",
        "An AI used for real-time facial recognition in {location}.",
        "An AI that manipulates behavior using {manipulation_method}.",
        "A predictive policing system using {controversial_input}.",
        "An AI that targets {group} based on inferred beliefs."
    ],
    "high": [
        "An AI system used to assess candidates for {job_type} roles.",
        "A diagnostic AI used in {healthcare_setting}.",
        "AI controlling traffic flow in {critical_infrastructure}.",
        "An AI that approves or denies applications for {financial_service}.",
        "A biometric system deployed at {border_point}."
    ],
    "limited": [
        "An AI chatbot that helps users navigate {platform}.",
        "An AI app that detects {emotion} in voice messages.",
        "An assistant that suggests actions based on {user_input}.",
        "A plugin that flags {inappropriate_content}.",
        "A recommendation engine that adapts content for {user_type}."
    ],
    "minimal": [
        "An AI tool that auto-corrects {input_type} in text editors.",
        "An AI filter that adjusts {visual_effect} in photos.",
        "A game AI that reacts to {player_action}.",
        "An AI that recommends songs based on {music_preference}.",
        "A smart assistant that reads weather forecasts aloud."
    ]
}

# Fill-in values
fillers = {
    "data_source": ["online behavior", "social media use", "purchase history", "criminal records", "search history"],
    "location": ["public squares", "train stations", "schools", "airports", "parks"],
    "manipulation_method": ["subliminal audio", "neuro-targeted ads", "personalized nudges"],
    "controversial_input": ["social background", "zip code", "school performance"],
    "group": ["ethnic groups", "political dissidents", "activists"],

    "job_type": ["management", "public safety", "education", "healthcare", "finance"],
    "healthcare_setting": ["emergency rooms", "oncology clinics", "rural hospitals", "private clinics"],
    "critical_infrastructure": ["smart cities", "electric grids", "rail networks"],
    "financial_service": ["credit cards", "loans", "mortgages"],
    "border_point": ["airports", "seaports", "international checkpoints"],

    "platform": ["online banking", "e-commerce", "university portals", "social networks"],
    "emotion": ["stress", "sadness", "joy", "anger"],
    "user_input": ["calendar use", "emails", "recent behavior"],
    "inappropriate_content": ["hate speech", "harassment", "explicit media"],
    "user_type": ["children", "elderly", "language learners"],

    "input_type": ["grammar", "spelling", "punctuation"],
    "visual_effect": ["brightness", "contrast", "tone"],
    "player_action": ["combat choices", "navigation", "puzzle solving"],
    "music_preference": ["tempo", "mood", "genre"]
}

# Generator function
def generate_examples(template_list, n):
    examples = []
    for _ in range(n):
        t = random.choice(template_list)
        for k, v in fillers.items():
            if f"{{{k}}}" in t:
                t = t.replace(f"{{{k}}}", random.choice(v))
        examples.append(t)
    return examples

# Create 100 examples per category
data = {
    "text": [],
    "labels": [],
    "label_id": []
}

for category, label_id in zip(templates.keys(), range(4)):
    examples = generate_examples(templates[category], 100)
    data["text"].extend(examples)
    data["labels"].extend([category] * 100)
    data["label_id"].extend([label_id] * 100)

df_synth = pd.DataFrame(data)
df_synth = df_synth[["text", "labels"]]
df_synth

synth_label_map = {
    "unacceptable": 0,
    "high": 1,
    "limited": 2,
    "minimal": 3
}

df_synth["labels"] = df_synth["labels"].map(synth_label_map)
df_synth

"""# Training Model"""

# Tokenization
def preprocess(example):
    return {
        **tokenizer(example['text'], truncation=True, padding='max_length', max_length=512),
        "labels": example["labels"]
    }

import evaluate
import numpy as np

# Evaluation metric
accuracy = evaluate.load("accuracy")

def compute_metrics(pred):
    logits, labels = pred
    preds = np.argmax(logits, axis=-1)
    return accuracy.compute(predictions=preds, references=labels)

"""## Training EU AI Act Model"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "joelniklaus/legal-xlm-roberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id))

from datasets import Dataset

train_ds_eu = Dataset.from_pandas(df_eu_final)
tokenized_eu_train = train_ds_eu.map(preprocess, batched=True)

ds_synth = Dataset.from_pandas(df_synth)
tokenized_ds_synth = ds_synth.map(preprocess, batched=True)


ds_synth_split = ds_synth.train_test_split(test_size=0.3, seed=42)

tokenized_ds_synth_train = ds_synth_split['train'].map(preprocess, batched=True)
tokenized_ds_synth_val = ds_synth_split['test'].map(preprocess, batched=True)

import torch

model.to("cuda")

print(torch.cuda.get_device_name(0))
print("Is model on GPU?", next(model.parameters()).is_cuda)

from transformers import TrainingArguments, Trainer

# Training setup
training_args_phase1 = TrainingArguments(
    report_to="wandb",
    run_name="EU_AI_Act_Training",
    output_dir="./results_legal_pretrain",
    eval_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=12,
    weight_decay=0.01,
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    save_total_limit=2,
    fp16=True,
    warmup_steps=100,
    logging_dir="./logs_phase1"
)

trainer = Trainer(
    model=model,
    args=training_args_phase1,
    train_dataset=tokenized_eu_train,
    eval_dataset=tokenized_eu_train,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.save_model("/content/drive/MyDrive/Automating-Risk-Evaluation/legal_pretrained_model")

"""## Training Risk Classification Model"""

model_name = "/content/drive/MyDrive/Automating-Risk-Evaluation/legal_pretrained_model"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id))

from datasets import Dataset

train_ds_eu = Dataset.from_pandas(df_eu_final)
tokenized_eu_train = train_ds_eu.map(preprocess, batched=True)

ds_synth = Dataset.from_pandas(df_synth)
tokenized_ds_synth = ds_synth.map(preprocess, batched=True)


ds_synth_split = ds_synth.train_test_split(test_size=0.3, seed=42)

tokenized_ds_synth_train = ds_synth_split['train'].map(preprocess, batched=True)
tokenized_ds_synth_val = ds_synth_split['test'].map(preprocess, batched=True)

import torch

model.to("cuda")

print(torch.cuda.get_device_name(0))
print("Is model on GPU?", next(model.parameters()).is_cuda)

from transformers import TrainingArguments, Trainer

# Training setup
training_args_phase2 = TrainingArguments(
    report_to="wandb",
    run_name="Risk Classification_Training",
    output_dir="./results_phase2_risk_classification",
    eval_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=12,
    weight_decay=0.01,
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    save_total_limit=2,
    fp16=True,
    warmup_steps=100,
    logging_dir="./logs_phase2",
)

trainer = Trainer(
    model=model,
    args=training_args_phase2,
    train_dataset=tokenized_ds_synth_train,
    eval_dataset=tokenized_ds_synth_val,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.save_model("/content/drive/MyDrive/Automating-Risk-Evaluation/risk_trained_model")

"""# Experiment

## Load Trained Model
"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification

legal_model_name = "/content/drive/MyDrive/Automating-Risk-Evaluation/legal_pretrained_model"
legal_tokenizer = AutoTokenizer.from_pretrained(legal_model_name)
legal_model = AutoModelForSequenceClassification.from_pretrained(legal_model_name, num_labels=len(label2id))
legal_model.to("cuda")

risk_model_name = "/content/drive/MyDrive/Automating-Risk-Evaluation/risk_trained_model"
risk_tokenizer = AutoTokenizer.from_pretrained(risk_model_name)
risk_model = AutoModelForSequenceClassification.from_pretrained(risk_model_name, num_labels=len(label2id))
risk_model.to("cuda")

"""## Categoriser"""

def predict(text):
    inputs = risk_tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=512).to("cuda")
    outputs = risk_model(**inputs)
    pred = outputs.logits.argmax(dim=1).item()
    label_names = {0: "unacceptable", 1: "high", 2: "limited", 3: "minimal"}
    return label_names[pred]

"""## Clause Recommendation"""

import torch

def suggest_articles(text, top_k=3):
    inputs = legal_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to('cuda')
    with torch.no_grad():
        outputs = legal_model(**inputs)

    probs = torch.nn.functional.softmax(outputs.logits, dim=1)
    topk_probs, topk_indices = torch.topk(probs, k=top_k, dim=1)

    results = []
    for prob, idx in zip(topk_probs[0], topk_indices[0]):
        article = id2label.get(idx.item())
        results.append((article, prob.item()))

    return results

"""## XAI"""

from lime.lime_text import LimeTextExplainer
import torch

# Label mapping
label_names = {0: "unacceptable", 1: "high", 2: "limited", 3: "minimal"}

# Prediction function wrapper for LIME
def predict_proba(texts):
    inputs = risk_tokenizer(texts, return_tensors="pt", truncation=True, padding=True, max_length=512).to("cuda")
    with torch.no_grad():
        outputs = risk_model(**inputs)
    probs = torch.nn.functional.softmax(outputs.logits, dim=1)
    return probs.cpu().numpy()

# Initialize explainer
explainer = LimeTextExplainer(class_names=list(label_names.values()))

def generate_explanation(text):
    explanation = explainer.explain_instance(text, predict_proba, num_features=5)
    return explanation.as_list()

"""## Testing"""

sample_text = "An AI photo editor that changes lighting in videos."
exp = explainer.explain_instance(sample_text, predict_proba, num_features=6)

print("Predicted class:", predict(sample_text))
print("Recommendations to read:")
for article in suggest_articles(sample_text):
    print(" - ", article)
exp.show_in_notebook(text=sample_text)

sample_text = "An AI that scores individuals based on social media behavior to determine creditworthiness."
exp = explainer.explain_instance(sample_text, predict_proba, num_features=6)

print("Predicted class:", predict(sample_text))
for article in suggest_articles(sample_text):
    print(" - ", article)
exp.show_in_notebook(text=sample_text)

"""### Further testing with real examples"""

sample_text = "A government uses an AI system for social scoring, ranking citizens based on their behavior to determine access to services or opportunities."
exp = explainer.explain_instance(sample_text, predict_proba, num_features=6)

print("Predicted class:", predict(sample_text))
for article in suggest_articles(sample_text):
    print(" - ", article)
exp.show_in_notebook(text=sample_text)

sample_text = "A hospital deploys an AI-powered diagnostic tool to analyze medical images for cancer detection."
exp = explainer.explain_instance(sample_text, predict_proba, num_features=6)

print("Predicted class:", predict(sample_text))
for article in suggest_articles(sample_text):
    print(" - ", article)
exp.show_in_notebook(text=sample_text)

sample_text = "A company uses a generative AI chatbot (like ChatGPT) on its website to answer customer queries. "
exp = explainer.explain_instance(sample_text, predict_proba, num_features=6)

print("Predicted class:", predict(sample_text))
for article in suggest_articles(sample_text):
    print(" - ", article)
exp.show_in_notebook(text=sample_text)

sample_text = "An email platform employs an AI-filter to automatically sort unwanted messages."
exp = explainer.explain_instance(sample_text, predict_proba, num_features=6)

print("Predicted class:", predict(sample_text))
for article in suggest_articles(sample_text):
    print(" - ", article)
exp.show_in_notebook(text=sample_text)

"""# User Interface"""

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from lime.lime_text import LimeTextExplainer

def classify_ai(text):
    predicted_class = predict(text)
    recommendations = suggest_articles(text)
    explanation = generate_explanation(text)
    return predicted_class, recommendations, explanation

with gr.Blocks() as demo:
    gr.Markdown("# AI Compliance Risk Categorizer\nEnter your AI system description below:")

    with gr.Row():
        input_text = gr.Textbox(lines=4, placeholder="Describe your AI system...")
        submit_btn = gr.Button("Predict & Recommend")

    output_label = gr.Label(label="Predicted Risk Category")
    output_recommendations = gr.List(label="Recommended Articles to Review")
    output_explanation = gr.Dataframe(headers=["Feature", "Weight"], label="Explanation (Top Features)")

    submit_btn.click(classify_ai, inputs=input_text, outputs=[output_label, output_recommendations, output_explanation])

demo.launch()
